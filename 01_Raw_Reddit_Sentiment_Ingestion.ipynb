{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61003249-595a-4788-9fcf-9d6f5be12ddb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Reddit API Data Ingestion Notebook\n",
    "--------------------------------------------------------------------------------\n",
    "## Description\n",
    "\n",
    "This notebook collects recent Reddit posts from popular cryptocurrency subreddits using the Reddit API. It filters posts based on relevant crypto keywords, structures the data, and writes the results as JSON files to a raw landing volume for downstream processing.\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "-  Uses the `praw` library to access Reddit's API  \n",
    "-  Filters posts based on a configurable list of crypto-related keywords  \n",
    "-  Targets a curated list of popular crypto subreddits  \n",
    "-  Normalizes keywords for consistent matching  \n",
    "-  Writes raw JSON files to Databricks Volumes with timestamp-based unique filenames  \n",
    "-  Designed as the upstream source for the Reddit Sentiment DLT pipeline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78be1874-c163-4492-a96b-973f893bfc27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Install Required Libraries\n",
    "# --------------------------------------------------------------------------------\n",
    "%pip install praw\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f9cf8a6-e9d5-4b7d-8bc8-4ef07bd582fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Import Libraries\n",
    "# --------------------------------------------------------------------------------\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "from datetime import datetime\n",
    "import praw\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Initialize Reddit API Client\n",
    "# --------------------------------------------------------------------------------\n",
    "# Note: For production, store these credentials securely using Databricks secrets.\n",
    "\n",
    "client_id = \"zpQTW6qW_vFnhZJxgqU3BA\"\n",
    "client_secret = \"EUEH0gvL7hErfu890AQ2aRJA-cdHFA\"\n",
    "user_agent = \"Crypto Sentiment Tracker\"\n",
    "\n",
    "reddit = praw.Reddit(client_id=client_id, client_secret=client_secret, user_agent=user_agent)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Notebook Parameter: Post Limit Per Subreddit (Define notebook widget)\n",
    "# --------------------------------------------------------------------------------\n",
    "dbutils.widgets.text(\"limit_reddit\", \"500\", \"Limit per subreddit\")\n",
    "limit = int(dbutils.widgets.get(\"limit_reddit\"))\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Define Keywords and Target Subreddits\n",
    "# --------------------------------------------------------------------------------\n",
    "keywords = [\n",
    "    \"bitcoin\", \"btc\",\n",
    "    \"ethereum\", \"eth\",\n",
    "    \"binance\", \"bnb\",\n",
    "    \"solana\", \"sol\",\n",
    "    \"ripple\", \"xrp\",\n",
    "    \"dogecoin\", \"doge\",\n",
    "    \"cardano\", \"ada\",\n",
    "    \"polkadot\", \"dot\"\n",
    "]\n",
    "\n",
    "# Normalize keywords to lowercase for consistency\n",
    "keywords = [kw.lower() for kw in keywords]\n",
    "\n",
    "subreddits = [\"CryptoCurrency\", \"Bitcoin\", \"CryptoMarkets\", \"Ethereum\", \"Dogecoin\", \"Altcoin\", \"DeFi\", \"BitcoinBeginners\",\n",
    "    \"NFT\", \"CryptoMoonShots\"]\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Collect Reddit Posts Matching Keywords\n",
    "# --------------------------------------------------------------------------------\n",
    "posts = []\n",
    "for sub in subreddits:\n",
    "    for submission in reddit.subreddit(sub).new(limit=limit):\n",
    "        title_lower = submission.title.lower()\n",
    "        if any(kw in title_lower for kw in keywords):\n",
    "            matched_kw = next((kw for kw in keywords if kw in title_lower), \"unknown\")\n",
    "            posts.append((\n",
    "                submission.id,\n",
    "                submission.title,\n",
    "                datetime.utcfromtimestamp(submission.created_utc),\n",
    "                sub,\n",
    "                matched_kw\n",
    "            ))\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Create Spark DataFrame from Collected Posts\n",
    "# --------------------------------------------------------------------------------\n",
    "schema = StructType([\n",
    "    StructField(\"post_id\", StringType()),\n",
    "    StructField(\"text\", StringType()),\n",
    "    StructField(\"created_utc\", TimestampType()),\n",
    "    StructField(\"subreddit\", StringType()),\n",
    "    StructField(\"keyword\", StringType())\n",
    "])\n",
    "\n",
    "spark_df = spark.createDataFrame(posts, schema=schema)\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Write Data to Raw Landing Volume (JSON Format)\n",
    "# --------------------------------------------------------------------------------\n",
    "# Filename includes UTC timestamp to ensure uniqueness\n",
    "\n",
    "volume_path = \"/Volumes/tabular/dataexpert/sakethg/capstone/raw/reddit_sentiment\"\n",
    "timestamp_str = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "unique_filename = f\"reddit_sentiment_{timestamp_str}.json\"\n",
    "full_path = f\"{volume_path}/{unique_filename}\"\n",
    "\n",
    "spark_df.write.mode(\"overwrite\").json(full_path)\n",
    "\n",
    "print(f\"Reddit sentiment data written to {full_path}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_Raw_Reddit_Sentiment_Ingestion",
   "widgets": {
    "limit_reddit": {
     "currentValue": "500",
     "nuid": "8f4d94a5-b0e2-4b26-9005-5ee878336777",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "500",
      "label": "Limit per subreddit",
      "name": "limit_reddit",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "500",
      "label": "Limit per subreddit",
      "name": "limit_reddit",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
